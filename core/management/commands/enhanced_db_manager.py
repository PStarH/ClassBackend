"""
æ”¹è¿›çš„æ•°æ®åº“ç®¡ç†å‘½ä»¤
"""
from django.core.management.base import BaseCommand, CommandError
from django.db import connection, transaction
from django.core.cache import cache
from django.utils import timezone
from datetime import timedelta
import logging
import time

from core.performance.optimizers import (
    query_profiler, connection_monitor, maintenance_scheduler,
    QueryProfiler, IndexAnalyzer, ConnectionPoolMonitor
)
from core.performance.cache_strategies import (
    adaptive_cache_manager, metrics_collector, warmup_manager
)

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = 'æ•°æ®åº“å’Œç¼“å­˜ä¼˜åŒ–ç®¡ç†å·¥å…·'

    def add_arguments(self, parser):
        parser.add_argument(
            'action',
            choices=[
                'health-check', 'optimize', 'cleanup', 'analyze',
                'cache-stats', 'warmup', 'monitor', 'maintenance'
            ],
            help='è¦æ‰§è¡Œçš„æ“ä½œ'
        )
        
        parser.add_argument(
            '--tables',
            nargs='*',
            help='æŒ‡å®šè¦æ“ä½œçš„è¡¨åï¼ˆç”¨äºmaintenanceæ“ä½œï¼‰'
        )
        
        parser.add_argument(
            '--force',
            action='store_true',
            help='å¼ºåˆ¶æ‰§è¡Œæ“ä½œ'
        )
        
        parser.add_argument(
            '--verbose',
            action='store_true',
            help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯'
        )
        
        parser.add_argument(
            '--threshold',
            type=float,
            default=1.0,
            help='æ…¢æŸ¥è¯¢é˜ˆå€¼ï¼ˆç§’ï¼‰'
        )

    def handle(self, *args, **options):
        action = options['action']
        
        self.stdout.write(f"ğŸš€ æ‰§è¡Œæ•°æ®åº“ä¼˜åŒ–æ“ä½œ: {action}")
        
        try:
            if action == 'health-check':
                self.health_check(options)
            elif action == 'optimize':
                self.optimize_database(options)
            elif action == 'cleanup':
                self.cleanup_database(options)
            elif action == 'analyze':
                self.analyze_performance(options)
            elif action == 'cache-stats':
                self.show_cache_stats(options)
            elif action == 'warmup':
                self.warmup_cache(options)
            elif action == 'monitor':
                self.monitor_system(options)
            elif action == 'maintenance':
                self.database_maintenance(options)
            
            self.stdout.write(
                self.style.SUCCESS(f"âœ… æ“ä½œ '{action}' å®Œæˆ")
            )
            
        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f"âŒ æ“ä½œå¤±è´¥: {str(e)}")
            )
            raise CommandError(f"Operation failed: {e}")

    def health_check(self, options):
        """å¥åº·æ£€æŸ¥"""
        self.stdout.write("ğŸ¥ æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
        
        # æ•°æ®åº“è¿æ¥æ£€æŸ¥
        try:
            with connection.cursor() as cursor:
                cursor.execute("SELECT 1")
                self.stdout.write("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}"))
        
        # ç¼“å­˜æ£€æŸ¥
        try:
            test_key = f'health_check_{int(time.time())}'
            cache.set(test_key, 'test', 10)
            if cache.get(test_key) == 'test':
                cache.delete(test_key)
                self.stdout.write("âœ… ç¼“å­˜ç³»ç»Ÿæ­£å¸¸")
            else:
                self.stdout.write(self.style.WARNING("âš ï¸ ç¼“å­˜ç³»ç»Ÿå¼‚å¸¸"))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"âŒ ç¼“å­˜ç³»ç»Ÿå¤±è´¥: {e}"))
        
        # è¿æ¥æ± çŠ¶æ€
        conn_stats = connection_monitor.get_detailed_connection_stats()
        self.stdout.write(f"ğŸ“Š æ•°æ®åº“è¿æ¥ç»Ÿè®¡:")
        self.stdout.write(f"  - å½“å‰è¿æ¥æ•°: {conn_stats['current_connections']}")
        self.stdout.write(f"  - æœ€å¤§è¿æ¥æ•°: {conn_stats['max_connections']}")
        
        if conn_stats['long_running_queries']:
            self.stdout.write(self.style.WARNING(
                f"âš ï¸ å‘ç° {len(conn_stats['long_running_queries'])} ä¸ªé•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢"
            ))
        
        # ç¼“å­˜æŒ‡æ ‡
        cache_metrics = metrics_collector.get_summary()
        self.stdout.write(f"ğŸ“ˆ ç¼“å­˜æŒ‡æ ‡:")
        self.stdout.write(f"  - å‘½ä¸­ç‡: {cache_metrics['hit_rate']:.2%}")
        self.stdout.write(f"  - è¯·æ±‚/ç§’: {cache_metrics['requests_per_second']:.2f}")

    def optimize_database(self, options):
        """ä¼˜åŒ–æ•°æ®åº“"""
        self.stdout.write("ğŸ”§ å¼€å§‹æ•°æ®åº“ä¼˜åŒ–...")
        
        with connection.cursor() as cursor:
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stdout.write("ğŸ“Š æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯...")
            cursor.execute("ANALYZE;")
            
            # æ¸…ç†è¡¨ç©ºé—´
            self.stdout.write("ğŸ§¹ æ¸…ç†è¡¨ç©ºé—´...")
            cursor.execute("VACUUM;")
            
            # ç´¢å¼•åˆ†æ
            self.stdout.write("ğŸ” åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ...")
            missing_indexes = IndexAnalyzer.analyze_missing_indexes()
            unused_indexes = IndexAnalyzer.analyze_unused_indexes()
            
            if missing_indexes:
                self.stdout.write("ğŸ’¡ å»ºè®®åˆ›å»ºçš„ç´¢å¼•:")
                for suggestion in missing_indexes[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                    self.stdout.write(f"  - {suggestion['suggestion']}")
            
            if unused_indexes:
                self.stdout.write("ğŸ—‘ï¸ å¯èƒ½éœ€è¦åˆ é™¤çš„ç´¢å¼•:")
                for suggestion in unused_indexes[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                    self.stdout.write(f"  - {suggestion['suggestion']}")

    def cleanup_database(self, options):
        """æ¸…ç†æ•°æ®åº“"""
        self.stdout.write("ğŸ§¹ å¼€å§‹æ•°æ®åº“æ¸…ç†...")
        
        # æ¸…ç†è¿‡æœŸä¼šè¯
        from apps.authentication.models import UserSession
        
        threshold = timezone.now() - timedelta(days=30)
        expired_sessions = UserSession.objects.filter(
            last_activity__lt=threshold
        )
        
        count = expired_sessions.count()
        if count > 0:
            if options['force'] or self.confirm_action(f"åˆ é™¤ {count} ä¸ªè¿‡æœŸä¼šè¯"):
                expired_sessions.delete()
                self.stdout.write(f"âœ… å·²åˆ é™¤ {count} ä¸ªè¿‡æœŸä¼šè¯")
        
        # æ¸…ç†ç¼“å­˜ä¸­çš„è¿‡æœŸé¡¹
        self.stdout.write("ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜...")
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ Redisç‰¹å®šçš„æ¸…ç†é€»è¾‘
            cache.clear()
            self.stdout.write("âœ… ç¼“å­˜æ¸…ç†å®Œæˆ")
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"âš ï¸ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}"))

    def analyze_performance(self, options):
        """æ€§èƒ½åˆ†æ"""
        self.stdout.write("ğŸ“Š å¼€å§‹æ€§èƒ½åˆ†æ...")
        
        threshold = options.get('threshold', 1.0)
        
        # æ…¢æŸ¥è¯¢åˆ†æ
        slow_queries = query_profiler.get_slow_queries()
        if slow_queries:
            self.stdout.write(f"ğŸŒ å‘ç° {len(slow_queries)} ä¸ªæ…¢æŸ¥è¯¢:")
            for query in slow_queries[:5]:
                self.stdout.write(
                    f"  - {query.execution_time:.2f}s: {query.sql[:100]}..."
                )
        
        # æŸ¥è¯¢ç»Ÿè®¡
        query_stats = query_profiler.get_query_statistics()
        self.stdout.write("ğŸ“ˆ æŸ¥è¯¢ç»Ÿè®¡:")
        self.stdout.write(f"  - æ€»æŸ¥è¯¢æ•°: {query_stats['total_queries']}")
        self.stdout.write(f"  - æ…¢æŸ¥è¯¢æ•°: {query_stats['slow_queries']}")
        self.stdout.write(f"  - æ…¢æŸ¥è¯¢æ¯”ä¾‹: {query_stats['slow_query_percentage']:.2f}%")
        self.stdout.write(f"  - å¹³å‡æ‰§è¡Œæ—¶é—´: {query_stats['avg_execution_time']:.3f}s")
        
        # è¡¨å¤§å°åˆ†æ
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                    pg_total_relation_size(schemaname||'.'||tablename) as bytes
                FROM pg_tables 
                WHERE schemaname = 'public'
                ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
                LIMIT 10;
            """)
            
            self.stdout.write("ğŸ“¦ æœ€å¤§çš„è¡¨:")
            for row in cursor.fetchall():
                self.stdout.write(f"  - {row[1]}: {row[2]}")

    def show_cache_stats(self, options):
        """æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡"""
        self.stdout.write("ğŸ“Š ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:")
        
        # åŸºæœ¬æŒ‡æ ‡
        metrics = metrics_collector.get_summary()
        self.stdout.write(f"ğŸ“ˆ åŸºæœ¬æŒ‡æ ‡:")
        for key, value in metrics.items():
            if isinstance(value, float):
                self.stdout.write(f"  - {key}: {value:.3f}")
            else:
                self.stdout.write(f"  - {key}: {value}")
        
        # è‡ªé€‚åº”ç¼“å­˜ç»Ÿè®¡
        self.stdout.write(f"ğŸ¯ è‡ªé€‚åº”ç¼“å­˜:")
        self.stdout.write(f"  - è·Ÿè¸ªçš„é”®æ•°é‡: {len(adaptive_cache_manager.access_stats)}")
        
        # æ˜¾ç¤ºè®¿é—®æœ€é¢‘ç¹çš„é”®
        top_keys = adaptive_cache_manager.access_stats.most_common(5)
        if top_keys:
            self.stdout.write("ğŸ”¥ è®¿é—®æœ€é¢‘ç¹çš„é”®:")
            for key, count in top_keys:
                hit_rate = adaptive_cache_manager.calculate_hit_rate(key)
                self.stdout.write(f"  - {key[:50]}: {count} æ¬¡è®¿é—®, {hit_rate:.2%} å‘½ä¸­ç‡")

    def warmup_cache(self, options):
        """ç¼“å­˜é¢„çƒ­"""
        self.stdout.write("ğŸ”¥ å¼€å§‹ç¼“å­˜é¢„çƒ­...")
        
        # æ‰§è¡Œæ‰€æœ‰é¢„çƒ­ç­–ç•¥
        warmup_manager.execute_warmup()
        
        self.stdout.write("âœ… ç¼“å­˜é¢„çƒ­å®Œæˆ")

    def monitor_system(self, options):
        """ç³»ç»Ÿç›‘æ§"""
        self.stdout.write("ğŸ‘ï¸ å¯åŠ¨ç³»ç»Ÿç›‘æ§...")
        
        try:
            while True:
                # è¿æ¥çŠ¶æ€
                conn_stats = connection_monitor.get_detailed_connection_stats()
                
                # ç¼“å­˜æŒ‡æ ‡
                cache_metrics = metrics_collector.get_summary()
                
                # æ˜¾ç¤ºå®æ—¶çŠ¶æ€
                self.stdout.write(f"\rğŸ“Š è¿æ¥: {conn_stats['current_connections']}, "
                               f"ç¼“å­˜å‘½ä¸­ç‡: {cache_metrics['hit_rate']:.2%}, "
                               f"RPS: {cache_metrics['requests_per_second']:.1f}", 
                               ending='')
                
                time.sleep(5)
                
        except KeyboardInterrupt:
            self.stdout.write("\nğŸ›‘ ç›‘æ§å·²åœæ­¢")

    def database_maintenance(self, options):
        """æ•°æ®åº“ç»´æŠ¤"""
        self.stdout.write("ğŸ”§ æ‰§è¡Œæ•°æ®åº“ç»´æŠ¤...")
        
        tables = options.get('tables', [])
        force = options.get('force', False)
        
        if not tables:
            # è·å–æ‰€æœ‰ç”¨æˆ·è¡¨
            with connection.cursor() as cursor:
                cursor.execute("""
                    SELECT tablename 
                    FROM pg_tables 
                    WHERE schemaname = 'public'
                    ORDER BY tablename;
                """)
                tables = [row[0] for row in cursor.fetchall()]
        
        self.stdout.write(f"ğŸ¯ å¤„ç† {len(tables)} ä¸ªè¡¨...")
        
        for table in tables:
            self.stdout.write(f"ğŸ”§ ç»´æŠ¤è¡¨: {table}")
            
            operations = maintenance_scheduler.perform_maintenance(table, force)
            
            if operations:
                self.stdout.write(f"  âœ… æ‰§è¡Œäº†: {', '.join(operations)}")
            else:
                self.stdout.write(f"  â„¹ï¸ æ— éœ€ç»´æŠ¤")

    def confirm_action(self, message):
        """ç¡®è®¤æ“ä½œ"""
        response = input(f"{message}? (y/N): ").lower().strip()
        return response in ['y', 'yes']
